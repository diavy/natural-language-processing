{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3RvT8ZhkR2QY"
   },
   "source": [
    "# Sentiment Analysis with Bert\n",
    "\n",
    "In this notebook we will be using the transformer model, first introduced in [this](https://arxiv.org/abs/1706.03762) paper. Specifically, we will be using the **BERT** (Bidirectional Encoder Representations from Transformers) model from [this](https://arxiv.org/abs/1810.04805) paper. \n",
    "\n",
    "Transformer models are considerably larger than anything else covered in these tutorials. As such we are going to use the [transformers library](https://github.com/huggingface/transformers) to get pre-trained transformers and use them as our embedding layers. We will try two training methods:\n",
    "1. freeze (not train) the transformer and only train the remainder of the model which learns from the representations produced by the transformer. \n",
    "2. Train all models end-to-end \n",
    "\n",
    "In this case we will be using a multi-layer bi-directional GRU, however any model can learn from these representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EItbzmqeT1jC"
   },
   "source": [
    "## Introduction to Bert\n",
    "\n",
    "### History\n",
    "2018 was a breakthrough year in NLP. Transfer learning, particularly models like Allen AI’s ELMO, OpenAI’s Open-GPT, and Google’s BERT allowed researchers to smash multiple benchmarks with minimal task-specific fine-tuning and provided the rest of the NLP community with pretrained models that could easily (with less data and less compute time) be fine-tuned and implemented to produce state of the art results. Unfortunately, for many starting out in NLP and even for some experienced practicioners, the theory and practical application of these powerful models is still not well understood.\n",
    "\n",
    "### What is BERT?\n",
    "**BERT (Bidirectional Encoder Representations from Transformers)**, released in late 2018, is the model we will use in this tutorial to provide readers with a better understanding of and practical guidance for using transfer learning models in NLP. BERT is a method of pretraining language representations that was used to create models that NLP practicioners can then download and use for free. You can either use these models to extract high quality language features from your text data, or you can fine-tune these models on a specific task (classification, entity recognition, question answering, etc.) with your own data to produce state of the art predictions.\n",
    "\n",
    "### Advantages of Fine-Tuning\n",
    "In this tutorial, we will use BERT to train a text classifier. Specifically, we will take the pre-trained BERT model, add an untrained layer of neurons on the end, and train the new model for our classification task. Why do this rather than train a train a specific deep learning model (a CNN, BiLSTM, etc.) that is well suited for the specific NLP task you need?\n",
    "\n",
    "**Quicker Development**\n",
    "First, the pre-trained BERT model weights already encode a lot of information about our language. As a result, it takes much less time to train our fine-tuned model - it is as if we have already trained the bottom layers of our network extensively and only need to gently tune them while using their output as features for our classification task. In fact, the authors recommend only 2-4 epochs of training for fine-tuning BERT on a specific NLP task (compared to the hundreds of GPU hours needed to train the original BERT model or a LSTM from scratch!).\n",
    "\n",
    "**Less Data**\n",
    "In addition and perhaps just as important, because of the pre-trained weights this method allows us to fine-tune our task on a much smaller dataset than would be required in a model that is built from scratch. A major drawback of NLP models built from scratch is that we often need a prohibitively large dataset in order to train our network to reasonable accuracy, meaning a lot of time and energy had to be put into dataset creation. By fine-tuning BERT, we are now able to get away with training a model to good performance on a much smaller amount of training data.\n",
    "\n",
    "**Better Results**\n",
    "Finally, this simple fine-tuning procedure (typically adding one fully-connected layer on top of BERT and training for a few epochs) was shown to achieve state of the art results with minimal task-specific adjustments for a wide variety of tasks: **classification, language inference, semantic similarity, question answering**, etc. Rather than implementing custom and sometimes-obscure architetures shown to work well on a specific task, simply fine-tuning BERT is shown to be a better (or at least equal) alternative.\n",
    "\n",
    "### A Shift in NLP\n",
    "This shift to transfer learning parallels the same shift that took place in computer vision a few years ago. Creating a good deep learning network for computer vision tasks can take millions of parameters and be very expensive to train. Researchers discovered that deep networks learn hierarchical feature representations (simple features like edges at the lowest layers with gradually more complex features at higher layers). Rather than training a new network from scratch each time, the lower layers of a trained network with generalized image features could be copied and transfered for use in another network with a different task. It soon became common practice to download a pre-trained deep network and quickly retrain it for the new task or add additional layers on top - vastly preferable to the expensive process of training a network from scratch. For many, the introduction of deep pre-trained language models in 2018 (ELMO, BERT, ULMFIT, Open-GPT, etc.) signals the same shift to transfer learning in NLP that computer vision saw.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pPzL8ALTWYog"
   },
   "source": [
    "## Installing the Hugging Face Library\n",
    "\n",
    "Next, let’s install the transformers package from **Hugging Face** which will give us a pytorch interface for working with BERT. (This library contains interfaces for other pretrained language models like OpenAI’s GPT and GPT-2.) We’ve selected the pytorch interface because it strikes a nice balance between the high-level APIs (which are easy to use but don’t provide insight into how things work) and tensorflow code (which contains lots of details but often sidetracks us into lessons about tensorflow, when the purpose here is BERT!).\n",
    "\n",
    "At the moment, the Hugging Face library seems to be the most widely accepted and powerful pytorch interface for working with BERT. In addition to supporting a variety of different pre-trained transformer models, the library also includes pre-built modifications of these models suited to your specific task. For example, in this tutorial we will use `BertForSequenceClassification`. We may also try to build our own classification model upon the base Bert model.\n",
    "\n",
    "The library also includes task-specific classes for token classification, question answering, next sentence prediciton, etc. Using these pre-built classes simplifies the process of modifying BERT for your purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "colab_type": "code",
    "id": "Z0qZwhYJR0hL",
    "outputId": "dacbf6a9-cb13-4350-acdf-a6ea2f5117de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 2.9MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
      "\u001b[K     |████████████████████████████████| 870kB 12.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.47)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 20.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.47)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (2.6.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (0.15.2)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=c0bdbd4ad56d6c692fe1f90911211c7f12e974ebd9be1a8d3a143c7636da69fc\n",
      "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, sentencepiece, transformers\n",
      "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 transformers-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wIOEYog4YyLG"
   },
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lGBQ6LlRYNUp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QNlYt-k4ZNc1"
   },
   "source": [
    "### Tokenize and Indexing\n",
    "\n",
    "The transformer has already been trained with a specific vocabulary, which means we need to train with the exact same vocabulary and also tokenize our data in the same way that the transformer did when it was initially trained.\n",
    "\n",
    "Luckily, the transformers library has tokenizers for each of the transformer models provided. In this case we are using the BERT model which ignores casing (i.e. will lower case every word). We get this by loading the pre-trained `bert-base-uncased` tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "ciX2L0wKY4W4",
    "outputId": "64c4965b-e95b-4748-ceee-bb56b505b3fa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xbgnqp8oZ4oS"
   },
   "source": [
    "The `tokenizer` has a `vocab` attribute which contains the actual vocabulary we will be using. We can check how many tokens are in it by checking its length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JrnNlfVmZqqp",
    "outputId": "c0cf1136-e4bc-4179-918f-0b434c321922"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "um6H61ulaNGv"
   },
   "source": [
    "Using the tokenizer is as simple as calling `tokenizer.tokenize` on a string. This will tokenize and lower case the data in a way that is consistent with the pre-trained transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tVFrH4t1Z-zN",
    "outputId": "92e81b9a-c676-47b2-ed4b-ed65fa0cb8fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'how', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('Hello WORLD how ARE yoU?')\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOZiBkQXab1e"
   },
   "source": [
    "We can numericalize tokens using our vocabulary using `tokenizer.convert_tokens_to_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RqcVe2ylaa5g",
    "outputId": "c604178d-28ae-490c-de1f-9079509a2a28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7592, 2088, 2129, 2024, 2017, 1029]\n"
     ]
    }
   ],
   "source": [
    "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8q6C8px0ulP-"
   },
   "source": [
    "### Special formatting\n",
    "Before we fed the inputs to Bert model, we are required to:\n",
    "\n",
    "* Add special tokens to the start and end of each sentence.\n",
    "* Pad & truncate all sentences to a single constant length.\n",
    "* Explicitly differentiate real tokens from padding tokens with the “attention mask”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XB32Z0S6xFo0"
   },
   "source": [
    "There are two special tokens we need to add:\n",
    "* At the end of every sentence, we need to append the special [SEP] token.\n",
    "This token is an artifact of two-sentence tasks, where BERT is given two separate sentences and asked to determine something;\n",
    "* For classification tasks, we must prepend the special [CLS] token to the beginning of every sentence.\n",
    "\n",
    "This token `[CLS]` has special significance. BERT consists of 12 Transformer layers. Each transformer takes in a list of token embeddings, and produces the same number of embeddings on the output (but with the feature values changed, of course!).\n",
    "\n",
    "![](../figs/bert1.png)\n",
    "\n",
    "On the output of the final (12th) transformer, only the first embedding (corresponding to the `[CLS]` token) is used by the classifier. Also, because BERT is trained to only use this `[CLS]` token for classification, we know that the model has been motivated to encode everything it needs for the classification step into that single 768-value embedding vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FW44whluakw-",
    "outputId": "0350f44c-4c74-46f2-8528-c8b0c827b03b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [SEP] [PAD] [UNK]\n"
     ]
    }
   ],
   "source": [
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrwVjtJ8v1io"
   },
   "source": [
    "We can get the indexes of the special tokens by converting them using the vocabulary, or by explicitly getting them from the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-43SDzO-vS9S",
    "outputId": "a6be96db-2e53-471e-c137-8dca3dca1b80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "# init_token_idx = tokenizer.cls_token_id\n",
    "# eos_token_idx = tokenizer.sep_token_id\n",
    "# pad_token_idx = tokenizer.pad_token_id\n",
    "# unk_token_idx = tokenizer.unk_token_id\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SGltWKPkz1tM"
   },
   "source": [
    "Another thing we need to handle is that the model was trained on sequences with a **defined maximum length** - it does not know how to handle sequences longer than it has been trained on. We can get the maximum length of these input sizes by checking the `max_model_input_sizes` for the version of the transformer we want to use. In this case, it is 512 tokens. (However, the recommended maximum length is 128 by Bert authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-Q0chrIOwSzN",
    "outputId": "b775c769-4e8e-461b-fc16-a6fd9ff7a703"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "MAX_INPUT_LENGTH = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "\n",
    "print(MAX_INPUT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8J_kEGDYCojl"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35CL7AK62fpZ"
   },
   "source": [
    "<!-- Fortunately, the transfomer library provides a utility function `tokenizer.encode` to do all above for us: \n",
    "* tokenzing\n",
    "* numericalizing\n",
    "* adding `[CLS]` and `[SEP]`\n",
    "* truncating to maximum length. -->\n",
    "\n",
    "We define the following function to tokenize the original sentence and apply trucating to max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOSF9OiLxp9E"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_truncate(sentence, max_length):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TiXVrVSK0vUY"
   },
   "outputs": [],
   "source": [
    "# tokens = tokenizer.encode('Hello WORLD how ARE yoU?', max_length=MAX_LENGTH)\n",
    "\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MaVoSvlk5nrg"
   },
   "source": [
    "### Generate train/test datasets\n",
    "Now we define our fields. \n",
    "* The transformer expects the batch dimension to be first, so we set `batch_first = True`. \n",
    "* As we already have the vocabulary for our text, provided by the transformer we set `use_vocab = False` to tell torchtext that we'll be handling the vocabulary side of things. \n",
    "* We pass our `tokenize_and_truncate` function as the tokenizer. \n",
    "* The `preprocessing` argument is a function that takes in the example after it has been tokenized, this is where we will convert the tokens to their indexes. \n",
    "* Set the special tokens to their predifined index values, i.e. 100 instead of `[UNK]`. otherwise, the default str value will cause error. Therefore, **there's no need to add special tokens `[CLS]` and `[SEP]` to inputs any more, since the later batching process will automatically do it for us**.\n",
    "\n",
    "We define the label field as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ha_khMM834fR"
   },
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "\n",
    "TEXT = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = lambda x:tokenize_and_truncate(x, max_length=MAX_LENGTH),\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  #include_lengths = True\n",
    "                  init_token = init_token_idx,\n",
    "                  eos_token = eos_token_idx,\n",
    "                  pad_token = pad_token_idx,\n",
    "                  unk_token = unk_token_idx\n",
    "                  )\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dMgGK5WM7MpH"
   },
   "source": [
    "We load the data and create the validation splits as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "IR3ZhHQY7Hkv",
    "outputId": "c644b2c0-c27a-4336-bcfa-c0da3111e5af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:03<00:00, 23.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext import datasets\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Xf_djfBH7UE8",
    "outputId": "eccf2741-12ca-4bfb-b124-b23d13da9f6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MzcRZ7mF-3m_"
   },
   "source": [
    "We can check an example and ensure that the text has already been numericalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "epZcDB9A-2KQ",
    "outputId": "b351b00e-cb97-4ecc-9f96-5a376cd420ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [9826, 1010, 2073, 2064, 1045, 4088, 999, 2023, 2001, 1037, 2659, 5166, 1010, 27762, 6051, 2143, 1010, 2009, 2001, 2061, 18178, 2229, 2100, 2009, 2018, 2149, 2035, 21305, 2007, 7239, 2000, 2129, 3294, 2128, 7559, 5732, 2009, 2001, 999, 1996, 4690, 3554, 5019, 4694, 1005, 1056, 2130, 4690, 9590, 1010, 2027, 2020, 2652, 2105, 2007, 2070, 6081, 10689, 2027, 4149, 2012, 24547, 1011, 20481, 1998, 2035, 2027, 2020, 2725, 2001, 2074, 22653, 2000, 3046, 1998, 2191, 2009, 2298, 2066, 2027, 2020, 8084, 999, 999, 2033, 1998, 2026, 2155, 2001, 1999, 1996, 6888, 2005, 1037, 2428, 2204, 2895, 3185, 2028, 2154, 1010, 2061, 2057, 2787, 2000, 2175, 2000, 1996, 3573, 1998, 2298, 2005, 2028, 1010, 1998, 2045, 2009, 2001, 1996, 2387, 19392, 2479, 3185, 1012, 1045, 2812, 2009, 2246, 2061, 2307, 2021, 2043, 2057, 3427, 2009, 2012, 2188, 1045, 8134, 2351, 2044, 1996, 2034, 3496, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2821, 1998, 1996, 5436, 1997, 1996, 2143, 1010, 1996, 2466, 2604, 1010, 1996, 5896, 1010, 4385, 1012, 1012, 2001, 1037, 9129, 1997, 13044, 2008, 1045, 2123, 1005, 1056, 2130, 2113, 2339, 1996, 2472, 1998, 3135, 2130, 13842, 2037, 2051, 2437, 2009, 999, 999, 2021, 2065, 2017, 4148, 2000, 21811, 2588, 2023, 3185, 1012, 1012, 2079, 2025, 2131, 2009, 999, 999, 999, 999, 999], 'label': 'neg'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "VFZfCZkJ-9Nf",
    "outputId": "e5a75c6c-6a12-482c-b9f2-e5df34d1749b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['honestly', ',', 'where', 'can', 'i', 'begin', '!', 'this', 'was', 'a', 'low', 'budget', ',', 'horribly', 'acted', 'film', ',', 'it', 'was', 'so', 'che', '##es', '##y', 'it', 'had', 'us', 'all', 'bursting', 'with', 'laughter', 'to', 'how', 'completely', 're', '##tar', '##ded', 'it', 'was', '!', 'the', 'sword', 'fighting', 'scenes', 'weren', \"'\", 't', 'even', 'sword', 'fights', ',', 'they', 'were', 'playing', 'around', 'with', 'some', 'plastic', 'swords', 'they', 'bought', 'at', 'wal', '-', 'mart', 'and', 'all', 'they', 'were', 'doing', 'was', 'just', 'moaning', 'to', 'try', 'and', 'make', 'it', 'look', 'like', 'they', 'were', 'struggling', '!', '!', 'me', 'and', 'my', 'family', 'was', 'in', 'the', 'mood', 'for', 'a', 'really', 'good', 'action', 'movie', 'one', 'day', ',', 'so', 'we', 'decided', 'to', 'go', 'to', 'the', 'store', 'and', 'look', 'for', 'one', ',', 'and', 'there', 'it', 'was', 'the', 'saw', '##tooth', 'island', 'movie', '.', 'i', 'mean', 'it', 'looked', 'so', 'great', 'but', 'when', 'we', 'watched', 'it', 'at', 'home', 'i', 'practically', 'died', 'after', 'the', 'first', 'scene', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'oh', 'and', 'the', 'plot', 'of', 'the', 'film', ',', 'the', 'story', 'board', ',', 'the', 'script', ',', 'etc', '.', '.', 'was', 'a', 'bunch', 'of', 'garbage', 'that', 'i', 'don', \"'\", 't', 'even', 'know', 'why', 'the', 'director', 'and', 'producer', 'even', 'wasted', 'their', 'time', 'making', 'it', '!', '!', 'but', 'if', 'you', 'happen', 'to', 'stumble', 'upon', 'this', 'movie', '.', '.', 'do', 'not', 'get', 'it', '!', '!', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[0])['text'])\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-g9QilX3BeCd"
   },
   "source": [
    "Although we've handled the vocabulary for the text, we still need to build the vocabulary for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jh7otpP5_D02",
    "outputId": "7d6d99c4-707d-4a30-9e7e-a85d6bbe5782"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function _default_unk_index at 0x7f949e29f1e0>, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "LABEL.build_vocab(train_data)\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mx-yAPsmB7Ip"
   },
   "source": [
    "As before, we create the iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Veu-9rnjBBLD"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 ### 32 is recommended by Bert Authors, may try larger size, i.e. 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5kKMZ5SSECmc"
   },
   "outputs": [],
   "source": [
    "# for batch in train_iterator:\n",
    "#     print(batch)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bKenlWKdC0rD"
   },
   "source": [
    "## Build Model\n",
    "\n",
    "For this task, we need to modify the pre-trained BERT model to give outputs for classification. There're two ways to do it:\n",
    "* Build our own model upon the base Bert model implemented by the Library\n",
    "* Use the library built-in model. \n",
    "\n",
    "Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task, including classification, Q&A, sequence labeling, etc.\n",
    "Here, we'll follow the 2nd method since it is simpler. The 1st method is more flexible and we'll leave it for future discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QWuoCac9eDeI"
   },
   "source": [
    "We’ll be using `BertForSequenceClassification`. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
    "\n",
    "OK, let’s load BERT! There are a few different pre-trained BERT models available. `bert-base-uncased` means the version that has only lowercase letters (`uncased`) and is the smaller version of the two (“base” vs “large”).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZR8v1v5GMuN3"
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertForSequenceClassification, AdamW, BertConfig\n",
    "NB_CLASSES = 2\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = NB_CLASSES, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J6cYw9rCh0Ze"
   },
   "source": [
    "We can check how many parameters the model has and where do they come from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "J8NRVIePhExA",
    "outputId": "cee2877f-a77f-4688-9649-2a60ac2a30f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 109,483,778 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "lCUieOoQh37d",
    "outputId": "75977ee5-6b3b-4417-afbd-7dccc08907a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight True torch.Size([30522, 768])\n",
      "bert.embeddings.position_embeddings.weight True torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight True torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.weight True torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias True torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias True torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias True torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias True torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias True torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight True torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias True torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight True torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias True torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight True torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias True torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight True torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias True torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias True torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias True torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias True torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias True torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight True torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias True torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight True torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias True torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight True torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias True torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight True torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias True torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias True torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias True torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias True torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias True torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight True torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias True torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight True torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias True torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight True torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias True torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight True torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias True torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias True torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias True torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias True torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias True torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight True torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias True torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight True torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias True torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight True torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias True torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight True torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias True torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias True torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias True torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias True torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias True torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight True torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias True torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight True torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias True torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight True torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias True torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight True torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias True torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias True torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias True torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias True torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias True torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight True torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias True torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight True torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias True torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight True torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias True torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight True torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias True torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias True torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias True torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias True torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias True torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight True torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias True torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight True torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias True torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight True torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias True torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight True torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias True torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias True torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias True torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias True torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias True torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight True torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias True torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight True torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias True torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight True torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias True torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight True torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias True torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias True torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias True torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias True torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias True torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight True torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias True torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight True torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias True torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight True torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias True torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight True torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias True torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias True torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias True torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias True torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias True torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight True torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias True torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight True torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias True torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight True torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias True torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight True torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias True torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias True torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias True torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias True torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias True torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight True torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias True torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight True torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias True torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight True torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias True torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight True torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias True torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias True torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias True torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias True torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight True torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias True torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight True torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias True torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight True torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias True torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight True torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias True torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight True torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias True torch.Size([768])\n",
      "bert.pooler.dense.weight True torch.Size([768, 768])\n",
      "bert.pooler.dense.bias True torch.Size([768])\n",
      "classifier.weight True torch.Size([2, 768])\n",
      "classifier.bias True torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    #if param.requires_grad:\n",
    "    print (name, param.requires_grad, param.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wz1BawrxjP1M"
   },
   "source": [
    "Our standard models have under 5M, but this one has almost 110M! Luckily, most of these parameters are from the transformer and we will decide to train or not train those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HJC4MMh3iuFz"
   },
   "outputs": [],
   "source": [
    "FREEZE_BERT = False\n",
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = not FREEZE_BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f_K6HqwsjjKR"
   },
   "source": [
    "Now let's check the trainable parameters again and see it largely reduced if we decide to freeze the bert parameters. We can certainly take it off (by setting `FREEZE_BERT=False`) and see if there's any improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PKutww7tjgea",
    "outputId": "94786207-05a9-4e8a-a83b-1eae91a56c6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 109,483,778 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eiYfK5zgkaef"
   },
   "source": [
    "## Train and test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UP8Bg0EYko5H"
   },
   "source": [
    "### Optimizer and learning rate scheduler\n",
    "For the purposes of fine-tuning, the authors recommend choosing from the following values:\n",
    "\n",
    "* Batch size: 16, 32 (We chose 32 when creating our DataLoaders).\n",
    "* Learning rate (Adam): 5e-5, 3e-5, 2e-5 (We’ll use 2e-5).\n",
    "* Number of epochs: 2, 3, 4 (We’ll use 4).\n",
    "* The epsilon parameter `eps = 1e-8` is “a very small number to prevent any division by zero in the implementation” (from [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)).\n",
    "\n",
    "You can find the creation of the AdamW optimizer in run_glue.py [here](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KL0WGFJIj6YQ"
   },
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98QUFGjClenB"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "N_EPOCHS = 4\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_iterator) * N_EPOCHS\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IPx73PTfonb3"
   },
   "outputs": [],
   "source": [
    "#Place the model onto the GPU (if available)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-dINgpdoqjuG"
   },
   "source": [
    "### Training loop\n",
    "Next, we'll define functions for: calculating accuracy, performing a training epoch, performing an evaluation epoch and calculating how long a training/evaluation epoch takes.\n",
    "\n",
    "**Note**: No need to define *loss* function here, because `BertForSequenceClassification` model will output loss for you. If `num_label==1`, it uses `MSELoss` for regression; otherwise, `CrossEntropyLoss` for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eaCbJrHdp-ya"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    #round predictions to the closest integer\n",
    "    pred_flat = torch.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = y.flatten()\n",
    "    correct = (pred_flat == labels_flat).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qj-D34x2stWu"
   },
   "source": [
    "We need to compute a `masking` vector as input to `BertForSequenceClassification` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZZGwEnvkqste"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = batch.text\n",
    "        masks = inputs.ne(0).float()\n",
    "        labels = batch.label.long() ### required for the loss computation\n",
    "        \n",
    "        outputs = model(input_ids=inputs, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=masks, \n",
    "                    labels=labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        predictions = outputs[1]\n",
    "        #print(loss)\n",
    "        #print(predictions)\n",
    "        acc = binary_accuracy(predictions, labels)\n",
    "        #print(acc)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E6-OuUHl8Dh3"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            inputs = batch.text\n",
    "            masks = inputs.ne(0).float()\n",
    "            labels = batch.label.long() ### required for the loss computation\n",
    "            \n",
    "            outputs = model(input_ids=inputs, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=masks, \n",
    "                        labels=labels)\n",
    "            \n",
    "            loss = outputs[0]\n",
    "            predictions = outputs[1]\n",
    "            #print(loss)\n",
    "            #print(predictions)\n",
    "            acc = binary_accuracy(predictions, labels)\n",
    "            #print(acc)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oBJp3pnW8jrh"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J4BBQhB78kgo"
   },
   "source": [
    "Finally, we'll train our model. This takes considerably longer than any of the previous models due to the size of the transformer. Even though we are not training any of the transformer's parameters we still need to pass the data through the model which takes a considerable amount of time on a standard GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "JnLSgERXvN3y",
    "outputId": "100e38bb-2291-40ff-da52-af80b6c9333e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Epoch Time: 8m 20s\n",
      "\tTrain Loss: 0.277 | Train Acc: 88.29%\n",
      "\t Val. Loss: 0.223 |  Val. Acc: 90.98%\n",
      "Epoch: 2 | Epoch Time: 8m 21s\n",
      "\tTrain Loss: 0.146 | Train Acc: 94.78%\n",
      "\t Val. Loss: 0.232 |  Val. Acc: 91.48%\n",
      "Epoch: 3 | Epoch Time: 8m 21s\n",
      "\tTrain Loss: 0.069 | Train Acc: 97.87%\n",
      "\t Val. Loss: 0.269 |  Val. Acc: 91.59%\n",
      "Epoch: 4 | Epoch Time: 8m 20s\n",
      "\tTrain Loss: 0.032 | Train Acc: 99.23%\n",
      "\t Val. Loss: 0.321 |  Val. Acc: 91.56%\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "MODEL_PARAS_OBJ = 'bert-plain.pt'\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator)\n",
    "        \n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), MODEL_PARAS_OBJ)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "av4U4gORJSbP"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "With the test set prepared, we can apply our fine-tuned model to generate predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pUDf4XsWvViI",
    "outputId": "f6858e97-728c-471a-fa9c-50eab2fb8d4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.214 | Test Acc: 91.26%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(MODEL_PARAS_OBJ))\n",
    "test_loss, test_acc = evaluate(model, test_iterator)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f9niUvJ2LTGm"
   },
   "source": [
    "## Inference\n",
    "We'll then use the model to test the sentiment of some sequences. We tokenize the input sequence, trim it down to the maximum length, add the special tokens to either side, convert it to a tensor, add a fake batch dimension and then pass it through our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C7WBjtN-LW_3"
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(model, tokenizer, sentence):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    tokens = tokens[:MAX_LENGTH-2]\n",
    "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    mask = torch.ones_like(tensor).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    mask = mask.unsqueeze(0)\n",
    "    prediction = model(tensor, attention_mask=mask)\n",
    "    output = torch.nn.functional.softmax(prediction[0], dim=1).squeeze()[1]\n",
    "    return output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VISbJdNOM-_b",
    "outputId": "efbe1650-24e0-4e88-e9ab-f59e2485ac5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03746431693434715"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, tokenizer, \"This film is terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IykwXCs3NB5s"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "sentiment_analysis_naive_bert_pytorch.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
