{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rje31Ndesfpr"
   },
   "source": [
    "# PoS tagging with simple LSTM\n",
    "\n",
    "Sequence labeling is the basic task in NLP. To address it, we need to build sequence models where there is some sort of dependence through time between your inputs. The classical example of a sequence model is the Hidden Markov Model for part-of-speech tagging. Another example is the conditional random field. In neural network field, we'll be using a **recurrent neural network** (RNN) as they are commonly used in analysing sequences.\n",
    "\n",
    "A RNN is a network that maintains some kind of state. For example, its output could be used as part of the next input, so that information can propogate along as the network passes over the sequence. In the case of an LSTM, for each element in the sequence, there is a corresponding hidden state $h_t$, which in principle can contain information from arbitrary points earlier in the sequence. We can use the hidden state to predict words in a language model, part-of-speech tags, and a myriad of other things.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dLOWtw5cwEAc"
   },
   "source": [
    "## LSTM in PyTorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NqajHGie2YRn"
   },
   "source": [
    "### Explanation\n",
    "`nn.LSTM` in PyTorch applies a _multi-layer_ long short-term memory (LSTM) RNN to an input sequence.\n",
    "\n",
    "For each element in the input sequence, each layer computes the following function:\n",
    "\n",
    "\\begin{array}{ll} \\\\\n",
    "            i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n",
    "            f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n",
    "            g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\\n",
    "            o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n",
    "            c_t = f_t * c_{(t-1)} + i_t * g_t \\\\\n",
    "            h_t = o_t * \\tanh(c_t) \\\\\n",
    "        \\end{array}\n",
    "\n",
    "where $h_t$ and $c_t$ is the hidden state and cell state at time $t$; $x_t$ is the input at time $t$; $h_{(t-1)}$\n",
    "is the hidden state of the layer at time $t-1$, or the initial hidden\n",
    "state at time 0; $i_t$, $f_t$, $g_t$,\n",
    "$o_t$ are the input, forget, cell, and output gates, respectively.\n",
    "$\\sigma$ is the sigmoid function, and $*$ is the Hadamard product.\n",
    "\n",
    "In a multilayer LSTM, the input $x^{(l)}_t$ of the $l$-th layer\n",
    "($l >= 2$) is the hidden state $h^{(l-1)}_t$ of the previous layer multiplied by\n",
    "dropout $\\delta^{(l-1)}_t$ where each $\\delta^{(l-1)}_t$ is a Bernoulli random\n",
    "variable which is 0 with probability `dropout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tRFkHKgixhb8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GZ1XA_1ZHBVc"
   },
   "source": [
    "### Inputs\n",
    "The inputs to LSTM sequence are `input` and `(h_0, c_0)`. \n",
    "* `input` is a tensor of shape **(seq_len, batch, input_size)**: containing the features of the input sequence. \n",
    "* `h_0` is a tensor of shape **(num_layers * num_directions, batch, hidden_size)** containing the initial hidden state for **each** element in the batch. If the LSTM is bidirectional, num_directions should be 2, else it should be 1. \n",
    "* `c_0` is a tensor of shape **(num_layers * num_directions, batch, hidden_size)** containing the initial cell state for **each** element in the batch. \n",
    "\n",
    "If `(h_0, c_0)` is not provided, both **h_0** and **c_0** default to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A7PK8NAZx18D"
   },
   "source": [
    "Therefore, **Pytorch's LSTM expects\n",
    "all of its inputs to be 3D tensors**. \n",
    "The semantics of the axes of these\n",
    "tensors is important. \n",
    "The first axis is the sequence itself, the second indexes instances in the mini-batch, and the third indexes elements of the input. We haven't discussed mini-batching, so lets just ignore that and assume we will always have just 1 dimension on the second axis. If\n",
    "we want to run the sequence model over the sentence \"The cow jumped\",\n",
    "our input should look like:\n",
    "\\begin{align}\\begin{bmatrix}\n",
    "   \\overbrace{q_\\text{The}}^\\text{row vector} \\\\\n",
    "   q_\\text{cow} \\\\\n",
    "   q_\\text{jumped}\n",
    "   \\end{bmatrix}\\end{align}\n",
    "\n",
    "**Except remember there is an additional 2nd dimension with size 1**.\n",
    "Let's see a quick example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "t5X17zX1xk7w",
    "outputId": "1aa13033-fdd4-4253-ef48-a8675e526ea9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0732,  0.0331,  0.0890]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[-0.0732,  0.0331,  0.0890]]], grad_fn=<StackBackward>), tensor([[[-0.4125,  0.1233,  0.7861]]], grad_fn=<StackBackward>))\n",
      "tensor([[[0.0403, 0.1489, 0.1011]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[0.0403, 0.1489, 0.1011]]], grad_fn=<StackBackward>), tensor([[[0.2402, 0.6963, 0.6919]]], grad_fn=<StackBackward>))\n",
      "tensor([[[0.1820, 0.2100, 0.1130]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[0.1820, 0.2100, 0.1130]]], grad_fn=<StackBackward>), tensor([[[0.3204, 0.4610, 0.2960]]], grad_fn=<StackBackward>))\n",
      "tensor([[[0.1399, 0.1771, 0.0479]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[0.1399, 0.1771, 0.0479]]], grad_fn=<StackBackward>), tensor([[[0.3080, 0.4618, 0.1557]]], grad_fn=<StackBackward>))\n",
      "tensor([[[ 0.0352,  0.0939, -0.0434]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[ 0.0352,  0.0939, -0.0434]]], grad_fn=<StackBackward>), tensor([[[ 0.0604,  0.2508, -0.1779]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
    "\n",
    "# initialize the hidden state.\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "    print(out)\n",
    "    print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HNcZ0PvgIg_3"
   },
   "source": [
    "### Outputs\n",
    "The outputs from LSTM sequence are `output` and `(h_n, c_n)`. \n",
    "* `output` is a tensor of shape **(seq_len, batch, num_directions * hidden_size)** containing the output features $h_t$ from the **last layer** of the LSTM, for each $t$. If a `torch.nn.utils.rnn.PackedSequence` has been given as the input, the output will also be a packed sequence. \n",
    "For the unpacked case, the directions can be separated using **output.view(seq_len, batch, num_directions, hidden_size)**, with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case. \n",
    "* `h_n` is a tensor of shape **(num_layers * num_directions, batch, hidden_size)** containing the hidden state for *t = seq_len*.\n",
    "Like output, the layers can be separated using **h_n.view(num_layers, num_directions, batch, hidden_size)** and similarly for `c_n`. \n",
    "\n",
    "* `c_n` is a tensor of shape **(num_layers * num_directions, batch, hidden_size)** containing the cell state for *t = seq_len*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QvKG-MXELx6n"
   },
   "source": [
    "**Note about packed sequence**:To deal with training sequence examples with different lengths, in addtion to *padding*, pytorch allows us to further *pack* the sequence into a tuple of *two lists*. One contains the elements of sequences, and other contains the batch size at each step. This is helpful in recovering the actual sequences as well as telling RNN what is the batch size at each time step. Please check an example [here](https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jJsi_AikyAgM"
   },
   "source": [
    "Insead of using a FOR loop as is shown above, we can do the entire sequence all at once.\n",
    "the first value returned by LSTM is all of the hidden states throughout\n",
    "the sequence. the second is just the most recent hidden state\n",
    "(compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "The reason for this is that:\n",
    "\"out\" will give you access to all hidden states in the sequence\n",
    "\"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "by passing it as an argument  to the lstm at a later time\n",
    "Add the extra 2nd dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "XWwSNshRyOXR",
    "outputId": "3b136c93-c84c-49bf-81eb-93471e999ea5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1930,  0.1003, -0.1436]],\n",
      "\n",
      "        [[ 0.0378,  0.1532, -0.0980]],\n",
      "\n",
      "        [[ 0.1615,  0.2071, -0.2197]],\n",
      "\n",
      "        [[ 0.1152,  0.1601, -0.1457]],\n",
      "\n",
      "        [[-0.0016,  0.0865, -0.1286]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[-0.0016,  0.0865, -0.1286]]], grad_fn=<StackBackward>), tensor([[[-0.0026,  0.2435, -0.5242]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PZGJzdbSvvIH"
   },
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HrfBCyAUozod"
   },
   "source": [
    "### Read and parse data\n",
    "The data used here is *Universal Dependencies Version 2 POS Tagged data*, which is included in `torchtext.datasets`. We define three `Field` object for `text`, `udtags` and `ptbtags`. We use `text` and `udtags` here but not `ptbtags` here, because it `ptbtags` is for chunking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qTF55N9XseE_"
   },
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "SEED = 1234\n",
    "#random.seed(SEED)\n",
    "#np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(lower = True)\n",
    "UD_TAGS = data.Field(unk_token = None)\n",
    "PTB_TAGS = data.Field(unk_token = None)\n",
    "\n",
    "fields = ((\"text\", TEXT), (\"udtags\", UD_TAGS), (\"ptbtags\", PTB_TAGS))\n",
    "train_data, valid_data, test_data = datasets.UDPOS.splits(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3hjmJZOorDw"
   },
   "source": [
    "Let's check the data examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Ehavonk4XOci",
    "outputId": "3ac67ecd-ff48-4ed8-b16e-a622a4da24a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 12543\n",
      "Number of validation examples: 2002\n",
      "Number of testing examples: 2077\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "HbkacIM3pFjp",
    "outputId": "96f396df-5e19-4232-f662-f34e797578e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['al', '-', 'zaman', ':', 'american', 'forces', 'killed', 'shaikh', 'abdullah', 'al', '-', 'ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'qaim', ',', 'near', 'the', 'syrian', 'border', '.'], 'udtags': ['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT'], 'ptbtags': ['NNP', 'HYPH', 'NNP', ':', 'JJ', 'NNS', 'VBD', 'NNP', 'NNP', 'NNP', 'HYPH', 'NNP', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', ',', 'IN', 'DT', 'JJ', 'NN', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IBrOWybup7Kw"
   },
   "source": [
    "### Build the vocabulary\n",
    "Here we used `glove` as word embedding inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Buldt-IapwsQ"
   },
   "outputs": [],
   "source": [
    "MIN_FREQ = 2\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 min_freq = MIN_FREQ,\n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "\n",
    "UD_TAGS.build_vocab(train_data)\n",
    "PTB_TAGS.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bb0-AVHrtXOs"
   },
   "source": [
    "summuraize certain statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "LDj_p6HMqK4Y",
    "outputId": "437d7b56-341f-42f3-ca29-9e48234621bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 8866\n",
      "Unique tokens in UD_TAG vocabulary: 18\n",
      "Unique tokens in PTB_TAG vocabulary: 51\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in UD_TAG vocabulary: {len(UD_TAGS.vocab)}\")\n",
    "print(f\"Unique tokens in PTB_TAG vocabulary: {len(PTB_TAGS.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "80pY39-HtEgV",
    "outputId": "aef6fd4e-d60c-4f68-f0f5-630de6af3cb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 9076), ('.', 8640), (',', 7021), ('to', 5137), ('and', 5002), ('a', 3782), ('of', 3622), ('i', 3379), ('in', 3112), ('is', 2239), ('you', 2156), ('that', 2036), ('it', 1850), ('for', 1842), ('-', 1426), ('have', 1359), ('\"', 1296), ('on', 1273), ('was', 1244), ('with', 1216)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lf329JsOtMSl",
    "outputId": "d1cc7903-7008-4c20-b333-6ee079733e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'the', '.', ',', 'to', 'and', 'a', 'of', 'i']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "KyM5wrJntP2c",
    "outputId": "ccdd6a6f-0290-49e1-83fa-e66ee46f00f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', 'NOUN', 'PUNCT', 'VERB', 'PRON', 'ADP', 'DET', 'PROPN', 'ADJ', 'AUX', 'ADV', 'CCONJ', 'PART', 'NUM', 'SCONJ', 'X', 'INTJ', 'SYM']\n"
     ]
    }
   ],
   "source": [
    "print(UD_TAGS.vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_adFWzLl89f3"
   },
   "source": [
    "Notice the `<unk>` and `<pad>` tokens are included in `TEXT`, and `<pad>` token is included in `UD_TAGS`, when building the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "LDjerCCRtV12",
    "outputId": "046371aa-87d4-42fd-da70-f6285bc8000c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NOUN', 34781), ('PUNCT', 23679), ('VERB', 23081), ('PRON', 18577), ('ADP', 17638), ('DET', 16285), ('PROPN', 12946), ('ADJ', 12477), ('AUX', 12343), ('ADV', 10548), ('CCONJ', 6707), ('PART', 5567), ('NUM', 3999), ('SCONJ', 3843), ('X', 847), ('INTJ', 688), ('SYM', 599)]\n"
     ]
    }
   ],
   "source": [
    "print(UD_TAGS.vocab.freqs.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ompkb7Wktqx6"
   },
   "source": [
    "### Batch the data\n",
    "\n",
    "Use `BucketIterator` to batch data samples with similar size to reduce the padding efforts, and do the paddings (i.e. filling with `pad_idx`) within each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YT-cxtTTtm6e"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9RnYt---vvd0"
   },
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "33e0of13wEUf"
   },
   "source": [
    "### Define the model structure\n",
    "\n",
    "In this section, we will use an LSTM to get part of speech tags. We will\n",
    "not use Viterbi or Forward-Backward or anything like that, but as a\n",
    "(challenging) exercise to the reader, think about how Viterbi could be\n",
    "used after you have seen what is going on.\n",
    "\n",
    "The model is as follows: let our input sentence be\n",
    "$w_1, \\dots, w_M$, where $w_i \\in V$, our vocab. Also, let\n",
    "$T$ be our tag set, and $y_i$ the tag of word $w_i$.\n",
    "Denote our prediction of the tag of word $w_i$ by\n",
    "$\\hat{y}_i$.\n",
    "\n",
    "This is a structure prediction, model, where our output is a sequence\n",
    "$\\hat{y}_1, \\dots, \\hat{y}_M$, where $\\hat{y}_i \\in T$.\n",
    "\n",
    "To do the prediction, pass an LSTM over the sentence. Denote the hidden\n",
    "state at timestep $i$ as $h_i$. Also, assign each tag a\n",
    "unique index (like how we had word\\_to\\_ix in the word embeddings\n",
    "section). Then our prediction rule for $\\hat{y}_i$ is\n",
    "\n",
    "\\begin{align}\\hat{y}_i = \\text{argmax}_j \\  (\\log \\text{Softmax}(Ah_i + b))_j\\end{align}\n",
    "\n",
    "That is, take the log softmax of the affine map of the hidden state,\n",
    "and the predicted tag is the tag that has the maximum value in this\n",
    "vector. Note this implies immediately that the dimensionality of the\n",
    "target space of $A$ is $|T|$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aaucjuC3u18Q"
   },
   "outputs": [],
   "source": [
    "class RNNPoSTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim,\n",
    "                 n_layers, bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        # if padding_idx is specified, pads the output with the embedding vector \n",
    "        # at padding_idx (initialized to zeros) whenever it encounters the index.\n",
    "        # So, the embedding of padding_idx is alywas zeros during training \n",
    "        self.embedding= nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text = [seq_len, batch_size]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(text)) ## do we really need dropout here???\n",
    "        #embedded = [seq_len, batch_size, emb_dim]\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        #outputs = [seq_len, batch_size, hid_dim * n_directions]\n",
    "        #hidden/cell = [n_layers * n_directions, batch_size, hidden_dim]\n",
    "\n",
    "        predictions = self.fc(self.dropout(outputs))\n",
    "        #predictions = [seq_len, batch_size, output_dim]\n",
    "\n",
    "        return predictions \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rVOGpUIk5Ynu"
   },
   "source": [
    "Here we use a **bi-directional** and **two-layers** LSTM with **dropout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q44PocYf4zpD"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = len(UD_TAGS.vocab)\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = RNNPoSTagger(INPUT_DIM, \n",
    "                     EMBEDDING_DIM, \n",
    "                     HIDDEN_DIM, \n",
    "                     OUTPUT_DIM, \n",
    "                     N_LAYERS, \n",
    "                     BIDIRECTIONAL, \n",
    "                     DROPOUT, \n",
    "                     PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3HU8DjLf7EaB"
   },
   "source": [
    "### Model initialization and parameters check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PWxRnBM-Fwur"
   },
   "source": [
    "Initialize the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "g-EA-SDu6Zf5",
    "outputId": "7d42f47f-ac26-4104-93c4-281d3a377690"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNPoSTagger(\n",
       "  (embedding): Embedding(8866, 100, padding_idx=1)\n",
       "  (rnn): LSTM(100, 128, num_layers=2, bidirectional=True)\n",
       "  (fc): Linear(in_features=256, out_features=18, bias=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.1)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "FjuUJJsyII6N",
    "outputId": "4eb30fdb-6f25-4414-aaff-f7a711a883c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight True torch.Size([8866, 100])\n",
      "rnn.weight_ih_l0 True torch.Size([512, 100])\n",
      "rnn.weight_hh_l0 True torch.Size([512, 128])\n",
      "rnn.bias_ih_l0 True torch.Size([512])\n",
      "rnn.bias_hh_l0 True torch.Size([512])\n",
      "rnn.weight_ih_l0_reverse True torch.Size([512, 100])\n",
      "rnn.weight_hh_l0_reverse True torch.Size([512, 128])\n",
      "rnn.bias_ih_l0_reverse True torch.Size([512])\n",
      "rnn.bias_hh_l0_reverse True torch.Size([512])\n",
      "rnn.weight_ih_l1 True torch.Size([512, 256])\n",
      "rnn.weight_hh_l1 True torch.Size([512, 128])\n",
      "rnn.bias_ih_l1 True torch.Size([512])\n",
      "rnn.bias_hh_l1 True torch.Size([512])\n",
      "rnn.weight_ih_l1_reverse True torch.Size([512, 256])\n",
      "rnn.weight_hh_l1_reverse True torch.Size([512, 128])\n",
      "rnn.bias_ih_l1_reverse True torch.Size([512])\n",
      "rnn.bias_hh_l1_reverse True torch.Size([512])\n",
      "fc.weight True torch.Size([18, 256])\n",
      "fc.bias True torch.Size([18])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    #if param.requires_grad:\n",
    "    print (name, param.requires_grad, param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EwDdB5BD7OIZ",
    "outputId": "ee2a09a6-9f63-41de-b375-b18087f5a889"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,522,010 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RjkhVfYRFtz7"
   },
   "source": [
    "Inject glove-embeddings into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "J1lpHsYfFsyR",
    "outputId": "15f9dce7-5917-481c-853e-44d9504b79f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8866, 100])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "5jIj63qYGQHD",
    "outputId": "72d131f2-b3ca-4577-afe1-4e832e1ae0f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
       "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 0.9261,  2.3049,  0.5502,  ..., -0.3492, -0.5298, -0.1577],\n",
       "        [-0.5972,  0.0471, -0.2406,  ..., -0.9446, -0.1126, -0.2260],\n",
       "        [-0.4809,  2.5629,  0.9530,  ...,  0.5278, -0.4588,  0.7294]])"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LTxqvqyGHLcD"
   },
   "source": [
    "Set up the embedding value of `<unk>` and `<pad>` to zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "tD_gBwghG_T6",
    "outputId": "3bd5d943-30c0-4269-9a1d-3b0fd73348e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> index is 0 , and <pad> index is 1\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
      "        ...,\n",
      "        [ 0.9261,  2.3049,  0.5502,  ..., -0.3492, -0.5298, -0.1577],\n",
      "        [-0.5972,  0.0471, -0.2406,  ..., -0.9446, -0.1126, -0.2260],\n",
      "        [-0.4809,  2.5629,  0.9530,  ...,  0.5278, -0.4588,  0.7294]])\n"
     ]
    }
   ],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(\"<unk> index is\", UNK_IDX, \", and <pad> index is\", PAD_IDX)\n",
    "print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TJcE-1psIXjx"
   },
   "source": [
    "## Model training and evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lHp9LoMpIiCt"
   },
   "source": [
    "### Set up optimizer and loss\n",
    "\n",
    "We use `Adam` here which is a standard optimizer for all neutral network models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TpJgLHzsHosR"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AZ5IGXp0LwOR"
   },
   "source": [
    "At each time step for a sequence, it is a classification task in essence. So we use `nn.CrossEntropyLoss` as the loss function. \n",
    "This criterion combines `nn.LogSoftmax` and `nn.NLLLoss` in one single class.\n",
    "\n",
    "It is useful when training a classification problem with `C` classes.\n",
    "If provided, the optional argument `weight` should be a 1D `Tensor`\n",
    "assigning weight to each of the classes.\n",
    "This is particularly useful when you have an unbalanced training set.\n",
    "\n",
    "The `input` is expected to contain raw, unnormalized scores for each class.\n",
    "`input` has to be a Tensor of size either **(minibatch, C)** or\n",
    "**(minibatch, C, d_1, d_2, ..., d_K)**\n",
    "with $K \\geq 1$ for the `K`-dimensional case (not used here).\n",
    "\n",
    "This criterion expects a class index in the range `[0, C-1]` as the\n",
    "`target` for each value of a 1D tensor of size `minibatch`; if `ignore_index`\n",
    "is specified, this criterion also accepts this class index (this index may not\n",
    "necessarily be in the class range).\n",
    "\n",
    "The loss of unweighted classes can be described as:\n",
    "${loss}(x, class) = -\\log\\left(\\frac{\\exp(x[class])}{\\sum_j \\exp(x[j])}\\right)= -x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right)$\n",
    "\n",
    "The losses are averaged across observations for each minibatch, and do not include `ignore_index` specified class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mxzBhEZzIvRm"
   },
   "outputs": [],
   "source": [
    "TAG_PAD_IDX = UD_TAGS.vocab.stoi[UD_TAGS.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YAZ5cxd8V4fN"
   },
   "source": [
    "move the `criterion` and `model` to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eNG-6bs65upJ"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAshwW2TWTqu"
   },
   "source": [
    "### Training process\n",
    "\n",
    "First, define a `categorical_accuracy` to compute the accuracy of predictions per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "961VznmTV-BF"
   },
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y, tag_pad_idx):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
    "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lj2merPFgUqy"
   },
   "source": [
    "Then, define the `train` and `evaluate` function, pay attention to the dimensions of input data, output predictions, and labels. Make sure that they match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FPkrDIC2dJCj"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, tag_pad_idx):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train() # turn of dropoff and augograd\n",
    "\n",
    "    for batch in iterator:\n",
    "        text = batch.text\n",
    "        tags = batch.udtags\n",
    "\n",
    "        # text = [sent_len, batch_size]\n",
    "        # tags = [sent_len, batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        predictions = model(text)\n",
    "        # predictions = [sent_len, batch_size, output_dim]\n",
    "        \n",
    "        ## reshape tensors to make them fit the loss function\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "\n",
    "        # predictions = [sent_len*batch_size, output_dim]\n",
    "        # tags = [sent_len*batch_size]\n",
    "\n",
    "        loss = criterion(predictions, tags)\n",
    "        acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "\n",
    "        loss.backward() ## back propagation\n",
    "        optimizer.step() ## update parameters\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nkxq0BGOl2WW"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, tag_pad_idx):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text = batch.text\n",
    "            tags = batch.udtags\n",
    "            \n",
    "            predictions = model(text)\n",
    "            \n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "            \n",
    "            loss = criterion(predictions, tags)\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WZF45sP1m46H"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xj5pmXUCnG7Z"
   },
   "source": [
    "Finally, run the training process as follows. Remeber to save the best model based on the metrics of validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "cwCT4imkm9-q",
    "outputId": "f8d591ae-3d77-4c8f-a763-5bfbb9c05b8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.918 | Train Acc: 71.42%\n",
      "\t Val. Loss: 0.575 |  Val. Acc: 82.35%\n",
      "Epoch: 2 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.345 | Train Acc: 89.14%\n",
      "\t Val. Loss: 0.480 |  Val. Acc: 85.15%\n",
      "Epoch: 3 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.262 | Train Acc: 91.66%\n",
      "\t Val. Loss: 0.432 |  Val. Acc: 86.30%\n",
      "Epoch: 4 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.220 | Train Acc: 93.01%\n",
      "\t Val. Loss: 0.409 |  Val. Acc: 87.93%\n",
      "Epoch: 5 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.194 | Train Acc: 93.76%\n",
      "\t Val. Loss: 0.388 |  Val. Acc: 88.27%\n",
      "Epoch: 6 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.172 | Train Acc: 94.44%\n",
      "\t Val. Loss: 0.377 |  Val. Acc: 88.43%\n",
      "Epoch: 7 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.154 | Train Acc: 95.02%\n",
      "\t Val. Loss: 0.367 |  Val. Acc: 88.70%\n",
      "Epoch: 8 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.140 | Train Acc: 95.47%\n",
      "\t Val. Loss: 0.370 |  Val. Acc: 88.90%\n",
      "Epoch: 9 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.130 | Train Acc: 95.78%\n",
      "\t Val. Loss: 0.353 |  Val. Acc: 89.05%\n",
      "Epoch: 10 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.119 | Train Acc: 96.09%\n",
      "\t Val. Loss: 0.364 |  Val. Acc: 89.03%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "MODEL_PARAS_OBJ = 'pos_lstm.pt'\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, TAG_PAD_IDX)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, TAG_PAD_IDX)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), MODEL_PARAS_OBJ)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "th5LoXsQoC-o"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "00Bfp0_onkUR",
    "outputId": "483952d2-3b15-4e07-8e94-c38fe746ffaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.365 |  Test Acc: 88.94%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, TAG_PAD_IDX)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XBsDaZRQodfj"
   },
   "source": [
    "## References\n",
    "* https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#example-an-lstm-for-part-of-speech-tagging\n",
    "* https://github.com/bentrevett/pytorch-pos-tagging/blob/master/1%20-%20Simple%20RNN%20PoS%20Tagger.ipynb"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pos_tagging_lstm_pytorch.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
