{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kCLpDlfqZ0k9"
   },
   "source": [
    "# PoS Tagging with BiLSTM-CRF model\n",
    "\n",
    "For this section, we will see a full, complicated example of a Bi-LSTM plus Conditional Random Field for pos tagging. The LSTM tagger is typically sufficient for part-of-speech tagging, but a sequence model like the CRF is really essential for strong performance. Although this name sounds scary, all the model is in essence a CRF but where an LSTM provides the features. This is an advanced model though, far more complicated than pure LSTM model. The key challenges of BiLSTM-CRF model, comparing with LSTM, are:\n",
    "* Write the recurrence for the **viterbi** variable at step $i$ for tag $k$.\n",
    "* Modify the recurrence to compute the **forward** variables instead.\n",
    "* Modify again the above recurrence to compute the forward variables in **log-space** (*hint: log-sum-exp*)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mlgCEiUodDwA"
   },
   "source": [
    "## Theoretical Background\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TuqhhEgY_5sG"
   },
   "source": [
    "### Intro to BiLSTM-CRF\n",
    "\n",
    "We'll explain Bi-LSTM+CRF model with a NER task. Assume that we have five labels: **B-Person, I-Person, B-Organization, I-Organization, O**. In addition, given a sentence $x$ consisting of five words (i.e. $w_0, w_1, w_2, w_3, w_4$), The inputs, outputs and structure of *BiLSTM* is shown as follows:\n",
    "\n",
    "![](../figs/CRF-LAYER-4.jpg)\n",
    "\n",
    "* First, every word in sentence $x$ is represented by a vector word embedding (may including character embedding as well). The word embedding usually is from a pre-trained word embedding file and the character embedding is initialized randomly.All the embeddings will be fine-tuned during the training process.\n",
    "* Second, the inputs of BiLSTM model are those embeddings and the outputs are predicted labels for words in sentence \n",
    "$x$\n",
    "\n",
    "Without **CRF** layer, **BiLSTM** itself is sufficient for sequence labeling task, by selecting the label which has the highest score for each word. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g6rTU-i89tbd"
   },
   "source": [
    "However, picking the maximum-scored label for each individual word does not gurantee that the whole predicted sequence is valid. Obviously, the outputs are as is shown abvoe, i.e. `I-Organization I-Person` and `B-Organization I-Person`. Adding **CRF** on top of **Bi-LSTM** could add some constrains to the final predicted labels to ensure they are valid. These constrains can be learned by the CRF layer automatically from the training dataset during the training process. For instance:\n",
    "* The label of the first word in a sentence should start with “B-“ or “O”, not “I-“\n",
    "* “B-label1 I-label2 I-label3 I-…”, in this pattern, label1, label2, label3 … should be the same named entity label.\n",
    "\n",
    "![](../figs/CRF-LAYER-2-v2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pudhk4NX9tbd"
   },
   "source": [
    "### Sequence score and loss function \n",
    "\n",
    "In the Bi-LSTM CRF, we define two kinds of potentials: **emission** and\n",
    "**transition**. The emission potential for the word at index $i$ comes\n",
    "from the hidden state of the Bi-LSTM at timestep $i$. We assume $\\textbf{P}_{y_i}$ represents the non-normalized score of emitting $y_i$ at index $i$;\n",
    "The transition scores are stored in a $|T|x|T|$ matrix\n",
    "$\\textbf{A}$, where $T$ is the tag set. We assume $\\textbf{A}_{j,k}$ is the score of transitioning\n",
    "to tag $j$ from tag $k$. So:\n",
    "\n",
    "\\begin{align}\\text{Score}(x,y) = \\sum_i \\log \\psi_\\text{EMIT}(y_i) + \\log \\psi_\\text{TRANS}(y_{i-1} \\rightarrow y_i)\\end{align}\n",
    "\n",
    "\\begin{align}= \\sum_{i=0} \\textbf{P}_{y_i} + \\textbf{A}_{y_i, y_{i-1}}\\end{align}\n",
    "\n",
    "CRF computes a conditional probability of a given sequence. Let\n",
    "$y$ be a tag sequence and $x$ an input sequence of words. According to softmax, then we compute\n",
    "\n",
    "\\begin{align}P(y|x) = \\frac{\\exp{(\\text{Score}(x, y)})}{\\sum_{y'} \\exp{(\\text{Score}(x, y')})}\\end{align}\n",
    "\n",
    "According maximum likelihood estimation, we get the loss function after taking logrithm of $P(y|x)$,\n",
    "\\begin{align}\n",
    "loss(\\textbf{P}, \\textbf{A}) = -logP(y|x)=log(\\sum_{y'} \\exp{(\\text{Score}(x, y')}) - Score(x,y)\n",
    "\\end{align}\n",
    "Once we have the above **loss** function, we're able to obtain model parameters through grdient decent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qgVE9IJteNI3"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nf4zJg--Z7sm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "SEED = 1234\n",
    "#random.seed(SEED)\n",
    "#np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(lower = True, batch_first=True )#, init_token='sos', eos_token='eos')\n",
    "UD_TAGS = data.Field(unk_token = None, init_token='<sos>', eos_token='<eos>', batch_first=True )\n",
    "PTB_TAGS = data.Field(unk_token = None, init_token='<sos>', eos_token='<eos>', batch_first=True )\n",
    "\n",
    "fields = ((\"text\", TEXT), (\"udtags\", UD_TAGS), (\"ptbtags\", PTB_TAGS))\n",
    "train_data, valid_data, test_data = datasets.UDPOS.splits(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "B7v6aHWipNeN",
    "outputId": "74cc88b6-6d94-4853-dc91-529edadef6de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 12543\n",
      "Number of validation examples: 2002\n",
      "Number of testing examples: 2077\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "bMaPPTs6p2j0",
    "outputId": "c5d8cdca-abc1-4d50-f4ce-fe09ab3ee68e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['al', '-', 'zaman', ':', 'american', 'forces', 'killed', 'shaikh', 'abdullah', 'al', '-', 'ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'qaim', ',', 'near', 'the', 'syrian', 'border', '.'], 'udtags': ['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT'], 'ptbtags': ['NNP', 'HYPH', 'NNP', ':', 'JJ', 'NNS', 'VBD', 'NNP', 'NNP', 'NNP', 'HYPH', 'NNP', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', ',', 'IN', 'DT', 'JJ', 'NN', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IEN8t6HHp44C"
   },
   "outputs": [],
   "source": [
    "MIN_FREQ = 2\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 min_freq = MIN_FREQ,\n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "\n",
    "UD_TAGS.build_vocab(train_data)\n",
    "PTB_TAGS.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "u-aCRL9RqDeN",
    "outputId": "e5efc7ad-19ca-4ee4-a860-ecfbd2f4cbc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 8866\n",
      "Unique tokens in UD_TAG vocabulary: 20\n",
      "Unique tokens in PTB_TAG vocabulary: 53\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in UD_TAG vocabulary: {len(UD_TAGS.vocab)}\")\n",
    "print(f\"Unique tokens in PTB_TAG vocabulary: {len(PTB_TAGS.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iDA3FC6SEyOZ",
    "outputId": "0de39663-c402-4067-b2cd-4c8710af9aab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', 'the', '.', ',', 'to', 'and', 'a', 'of', 'i']"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Kcuu6UCNIl25",
    "outputId": "abd0ca70-1240-46f0-a46d-b931846186d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<sos>', '<eos>', 'NOUN', 'PUNCT', 'VERB', 'PRON', 'ADP', 'DET', 'PROPN', 'ADJ', 'AUX', 'ADV', 'CCONJ', 'PART', 'NUM', 'SCONJ', 'X', 'INTJ', 'SYM']\n"
     ]
    }
   ],
   "source": [
    "print(UD_TAGS.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EC0JPZkIqMeZ"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    sort_key=lambda x: len(x.text), # field sorted by len\n",
    "    sort_within_batch=True,\n",
    "    batch_sizes = (BATCH_SIZE, BATCH_SIZE, BATCH_SIZE),\n",
    "    device = device\n",
    ")\n",
    "\n",
    "# train_iterator = data.BucketIterator(\n",
    "#     train_data,\n",
    "#     sort_key=lambda x: len(x.text), # field sorted by len\n",
    "#     sort_within_batch=True,\n",
    "#     batch_size = BATCH_SIZE,\n",
    "#     device = device\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tMaCDzpryem"
   },
   "source": [
    "## Build the Model\n",
    "\n",
    "The model consists of two components: CRF and RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VXzBXgqe4MZt"
   },
   "source": [
    "### CRF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aoH0haEd4vE3"
   },
   "source": [
    "First, define a helper function to compute the log sum exp of an input vector, in a numerically stable way for the forward algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vrW8Mn4r5Xep"
   },
   "outputs": [],
   "source": [
    "def log_sum_exp(x):\n",
    "    m = torch.max(x, -1)[0]\n",
    "    return m + torch.log(torch.sum(torch.exp(x - m.unsqueeze(-1)), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vCTs2tDc9tbw"
   },
   "source": [
    "The, we define a `CRF` model, with important class functions and variables as follows:\n",
    "\n",
    "* `self.trans` is the transition score matrix, with `self.trans[i][j]` reprenting the score of transiting from tag *j* to tag *i*\n",
    "<!-- * `self.neg_log_likelihood` corresponds to the loss function defined above;  -->\n",
    "* `self.score` computes $Score(x, y)$,\n",
    "* `self.forward` computes the log sum equation $log(\\sum_{y'} \\exp{(\\text{Score}(x, y')})$,\n",
    "<!-- * `self._get_lstm_features` computes the emission scores (i.e. $\\textbf{P}$) from BiLSTM, as the inputs to loss function  -->\n",
    "* `self.decode` uses **viterbi** algorithm to predict/decode new sentence, given that the model is already trained, i.e. $\\textbf{A}$ and $\\textbf{P}$ is known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "434jAwjv6IWe"
   },
   "outputs": [],
   "source": [
    "class CRF(nn.Module):\n",
    "    def __init__(self, num_tags):\n",
    "        super().__init__()\n",
    "        self.batch_size = 0\n",
    "        self.num_tags = num_tags\n",
    "\n",
    "        # matrix of transition scores from j to i\n",
    "        self.trans = nn.Parameter(torch.randn(num_tags, num_tags))\n",
    "        self.trans.data[SOS_IDX, :] = -10000 # no transition to SOS\n",
    "        self.trans.data[:, EOS_IDX] = -10000 # no transition from EOS except to PAD\n",
    "        self.trans.data[:, PAD_IDX] = -10000 # no transition from PAD except to PAD\n",
    "        self.trans.data[PAD_IDX, :] = -10000 # no transition to PAD except from EOS\n",
    "        self.trans.data[PAD_IDX, EOS_IDX] = 0\n",
    "        self.trans.data[PAD_IDX, PAD_IDX] = 0\n",
    "\n",
    "    def forward(self, h, mask):\n",
    "        \"\"\"\n",
    "        Partition function in forawrd algorithm\n",
    "        h = [batch_size, seq_len, num_tags]\n",
    "        mask = [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        # initialize forward variables in log space\n",
    "        score = torch.Tensor(self.batch_size, self.num_tags).fill_(-10000) # [batch_size, num_tags]\n",
    "        score = score.to(device) ### put it to GPU if available\n",
    "        # score = torch.zeros(self.batch_size, self.num_tags)# [batch_size, num_tags]\n",
    "        # score.fill_(-10000)\n",
    "        score[:, SOS_IDX] = 0.\n",
    "        trans = self.trans.unsqueeze(0) # [1, num_tags, num_tags]\n",
    "        for t in range(h.size(1)): # recursion through the sequence\n",
    "            mask_t = mask[:, t].unsqueeze(1) # [batch_size] -> [batch_size, 1]\n",
    "            emit_t = h[:, t].unsqueeze(2) # [batch_size, num_tags] -> [batch_size, num_tags, 1]\n",
    "            score_t = score.unsqueeze(1) + emit_t + trans # [batch_size, 1, num_tags] -> [batch_size, num_tags, num_tags]\n",
    "            score_t = log_sum_exp(score_t) # [batch_size, num_tags, num_tags] -> [batch_size, num_tags]\n",
    "            score = score_t * mask_t + score * (1 - mask_t)\n",
    "        score = log_sum_exp(score + self.trans[EOS_IDX]) ## start from <SOS> and stops at <EOS>\n",
    "        return score # partition function\n",
    "\n",
    "    def score(self, h, y0, mask): \n",
    "        \"\"\"\n",
    "        Calculate the score of a given sequence\n",
    "        y0 = [batch_size, 1+seq_len] ### start with <SOS>\n",
    "        h = [batch_size, seq_len, num_tags]\n",
    "        mask = [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        score = torch.Tensor(self.batch_size).fill_(0.)\n",
    "        score = score.to(device)\n",
    "        #score = torch.zeros(self.batch_size)\n",
    "        h = h.unsqueeze(3)  # [batch_size, seq_len, num_tags, 1]\n",
    "        trans = self.trans.unsqueeze(2) # [num_tags, num_tags, 1]\n",
    "        for t in range(h.size(1)): # recursion through the sequence\n",
    "            mask_t = mask[:, t] # [batch_size]\n",
    "            emit_t = torch.cat([h[t, y0[t + 1]] for h, y0 in zip(h, y0)]) # [batch_size]\n",
    "            trans_t = torch.cat([trans[y0[t + 1], y0[t]] for y0 in y0]) # [batch_size]\n",
    "            score += (emit_t + trans_t) * mask_t\n",
    "        last_tag = y0.gather(1, mask.sum(1).long().unsqueeze(1)).squeeze(1)\n",
    "        score += self.trans[EOS_IDX, last_tag]\n",
    "        return score\n",
    "\n",
    "    def decode(self, h, mask):\n",
    "        \"\"\"\n",
    "        Viterbi decoding\n",
    "        \"\"\"\n",
    "        # initialize backpointers and viterbi variables in log space\n",
    "        bptr = torch.LongTensor()\n",
    "        score = torch.Tensor(self.batch_size, self.num_tags).fill_(-10000)\n",
    "        bptr = bptr.to(device)\n",
    "        score = score.to(device)\n",
    "        # score = torch.zeros(self.batch_size, self.num_tags)# [batch_size, num_tags]\n",
    "        # score.fill_(-10000)\n",
    "        score[:, SOS_IDX] = 0.\n",
    "\n",
    "        for t in range(h.size(1)): # recursion through the sequence\n",
    "            mask_t = mask[:, t].unsqueeze(1)\n",
    "            score_t = score.unsqueeze(1) + self.trans # [B, 1, C] -> [B, C, C]\n",
    "            score_t, bptr_t = score_t.max(2) # best previous scores and tags\n",
    "            score_t += h[:, t] # plus emission scores\n",
    "            bptr = torch.cat((bptr, bptr_t.unsqueeze(1)), 1)\n",
    "            score = score_t * mask_t + score * (1 - mask_t)\n",
    "        score += self.trans[EOS_IDX]\n",
    "        best_score, best_tag = torch.max(score, 1)\n",
    "\n",
    "        # back-tracking\n",
    "        bptr = bptr.tolist()\n",
    "        best_path = [[i] for i in best_tag.tolist()]\n",
    "        for b in range(self.batch_size):\n",
    "            i = best_tag[b] # best tag\n",
    "            j = int(mask[b].sum().item())\n",
    "            for bptr_t in reversed(bptr[b][:j]):\n",
    "                i = bptr_t[i]\n",
    "                best_path[b].append(i)\n",
    "            best_path[b].pop()\n",
    "            best_path[b].reverse()\n",
    "\n",
    "        return best_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J7_hXMQMXX2K"
   },
   "source": [
    "### BiLSTM model. \n",
    "\n",
    "This part is relatively easy and should be almost the same as previous code example. The two major differences are:\n",
    "1. we set `batch_first=True` in the RNN constructor, so that the output of RNN is easily connect to the CRF component.\n",
    "2. we use `rnn.pack_padded_sequence` and `rnn.pad_packed_sequence` to handle padding sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nSpgeGC-XkqS"
   },
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_tags,\n",
    "                 n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.batch_size = 0\n",
    "        # if padding_idx is specified, pads the output with the embedding vector \n",
    "        # at padding_idx (initialized to zeros) whenever it encounters the index.\n",
    "        # So, the embedding of padding_idx is alywas zeros during training \n",
    "        self.embedding= nn.Embedding(vocab_size, embedding_dim, padding_idx=WORD_PAD_IDX)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional,\n",
    "                           batch_first = True) ### put batch_size at 0 dimension\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, num_tags)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, mask):\n",
    "        # text = [batch_size, seq_len]\n",
    "        # mask = [batch_size, seq_len]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(text)) ## do we really need dropout here???\n",
    "        #embedded = [batch_size, seq_len, emb_dim]\n",
    "\n",
    "        x = nn.utils.rnn.pack_padded_sequence(embedded, mask.sum(1).int(), batch_first = True)\n",
    "        h, _ = self.rnn(x)\n",
    "        h, _ = nn.utils.rnn.pad_packed_sequence(h, batch_first = True)\n",
    "        h = self.fc(self.dropout(h))\n",
    "        predictions = h * mask.unsqueeze(2)\n",
    "        #predictions = [batch_size, seq_len, num_tags]\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dW2PBNLYfnun"
   },
   "source": [
    "### Assembly RNN + CRF to BiLSTM-CRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V5HjHtuzfmEi"
   },
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_tags,\n",
    "                 n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.bilstm = BiLSTM(vocab_size, embedding_dim, hidden_dim, num_tags,\n",
    "                 n_layers, bidirectional, dropout)\n",
    "        self.crf = CRF(num_tags)\n",
    "        #self = self.cuda() if CUDA else self\n",
    "\n",
    "    def forward(self, xw, y0): # for training\n",
    "        #self.zero_grad()\n",
    "        self.bilstm.batch_size = y0.size(0)\n",
    "        self.crf.batch_size = y0.size(0)\n",
    "        mask = y0[:, 1:].ne(PAD_IDX).float() ### start from y0[1] to skip <SOS>\n",
    "        #mask = y0.ne(PAD_IDX).float()\n",
    "        h = self.bilstm(xw, mask)\n",
    "        Z = self.crf.forward(h, mask)\n",
    "        score = self.crf.score(h, y0, mask)\n",
    "        return torch.mean(Z - score) # average NLL loss of a mini-batch\n",
    "\n",
    "    def decode(self, xw): # for inference\n",
    "        self.bilstm.batch_size = xw.size(0)\n",
    "        self.crf.batch_size = xw.size(0)\n",
    "        mask = xw.ne(WORD_PAD_IDX).float()\n",
    "        h = self.bilstm(xw, mask)\n",
    "        return self.crf.decode(h, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K3HneEmXGUe6"
   },
   "source": [
    "### Create a model instance\n",
    "\n",
    "Here we add a `START_TAG` and `END_TAG` to the transition matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U58Ps202GTXD"
   },
   "outputs": [],
   "source": [
    "PAD_IDX = UD_TAGS.vocab.stoi[UD_TAGS.pad_token]\n",
    "SOS_IDX = UD_TAGS.vocab.stoi[UD_TAGS.init_token]\n",
    "EOS_IDX = UD_TAGS.vocab.stoi[UD_TAGS.eos_token]\n",
    "WORD_PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "NUM_TAGS = len(UD_TAGS.vocab) ### including <SOS> and <EOS>\n",
    "#OUTPUT_DIM = len(UD_TAGS.vocab)\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h73s-rnBN7Dr"
   },
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF(INPUT_DIM, \n",
    "                     EMBEDDING_DIM, \n",
    "                     HIDDEN_DIM, \n",
    "                     NUM_TAGS, \n",
    "                     N_LAYERS, \n",
    "                     BIDIRECTIONAL, \n",
    "                     DROPOUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84F_FTDsPoau"
   },
   "source": [
    "### Model initiaion and parameters check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "rRJLC76pPjQV",
    "outputId": "e2614bef-0f96-4a6a-e72b-bb7d7458a6c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM_CRF(\n",
       "  (bilstm): BiLSTM(\n",
       "    (embedding): Embedding(8866, 100, padding_idx=1)\n",
       "    (rnn): LSTM(100, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
       "    (fc): Linear(in_features=256, out_features=20, bias=True)\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (crf): CRF()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.1)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "A-_SKXg-P3HQ",
    "outputId": "d7b36791-398d-482e-b6db-cdf77f738b4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bilstm.embedding.weight True torch.Size([8866, 100])\n",
      "bilstm.rnn.weight_ih_l0 True torch.Size([512, 100])\n",
      "bilstm.rnn.weight_hh_l0 True torch.Size([512, 128])\n",
      "bilstm.rnn.bias_ih_l0 True torch.Size([512])\n",
      "bilstm.rnn.bias_hh_l0 True torch.Size([512])\n",
      "bilstm.rnn.weight_ih_l0_reverse True torch.Size([512, 100])\n",
      "bilstm.rnn.weight_hh_l0_reverse True torch.Size([512, 128])\n",
      "bilstm.rnn.bias_ih_l0_reverse True torch.Size([512])\n",
      "bilstm.rnn.bias_hh_l0_reverse True torch.Size([512])\n",
      "bilstm.rnn.weight_ih_l1 True torch.Size([512, 256])\n",
      "bilstm.rnn.weight_hh_l1 True torch.Size([512, 128])\n",
      "bilstm.rnn.bias_ih_l1 True torch.Size([512])\n",
      "bilstm.rnn.bias_hh_l1 True torch.Size([512])\n",
      "bilstm.rnn.weight_ih_l1_reverse True torch.Size([512, 256])\n",
      "bilstm.rnn.weight_hh_l1_reverse True torch.Size([512, 128])\n",
      "bilstm.rnn.bias_ih_l1_reverse True torch.Size([512])\n",
      "bilstm.rnn.bias_hh_l1_reverse True torch.Size([512])\n",
      "bilstm.fc.weight True torch.Size([20, 256])\n",
      "bilstm.fc.bias True torch.Size([20])\n",
      "crf.trans True torch.Size([20, 20])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    #if param.requires_grad:\n",
    "    print (name, param.requires_grad, param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "55IANsR0QkQE",
    "outputId": "a88be268-aa20-4296-f078-e50999b6ffd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,522,924 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "spHudlYbQuwl",
    "outputId": "2b27381f-0a3c-4321-adf0-4f094f051d91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8866, 100])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "J2AIHigjQ1TW",
    "outputId": "0ffd4e5b-9352-4eb8-9caf-4498007536fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
       "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 0.9261,  2.3049,  0.5502,  ..., -0.3492, -0.5298, -0.1577],\n",
       "        [-0.5972,  0.0471, -0.2406,  ..., -0.9446, -0.1126, -0.2260],\n",
       "        [-0.4809,  2.5629,  0.9530,  ...,  0.5278, -0.4588,  0.7294]])"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bilstm.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "3jBB9DP-Q32w",
    "outputId": "d4522028-1013-4b27-fa73-ffb6f70afb03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> index is 0 , and <pad> index is 1\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
      "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
      "        ...,\n",
      "        [ 0.9261,  2.3049,  0.5502,  ..., -0.3492, -0.5298, -0.1577],\n",
      "        [-0.5972,  0.0471, -0.2406,  ..., -0.9446, -0.1126, -0.2260],\n",
      "        [-0.4809,  2.5629,  0.9530,  ...,  0.5278, -0.4588,  0.7294]])\n"
     ]
    }
   ],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.bilstm.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.bilstm.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(\"<unk> index is\", UNK_IDX, \", and <pad> index is\", WORD_PAD_IDX)\n",
    "print(model.bilstm.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QyPjWQQWRSxB"
   },
   "source": [
    "## Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pBCXTgvXQ9kn"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NOLO8xiWRZRb"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "#criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zFjEMHYRSOgd"
   },
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_mrqi9qZR96T"
   },
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y, tag_pad_idx):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
    "    correct = preds[non_pad_elements].eq(y[non_pad_elements])\n",
    "    acc = correct.sum() / torch.tensor(len(y[non_pad_elements]), dtype=torch.float)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lqWAeA1ESmmU"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train() # turn of dropoff and augograd\n",
    "\n",
    "    for batch in iterator:\n",
    "        text = batch.text\n",
    "        tags = batch.udtags[:,:-1] ## keep <SOS> and strip <EOS> tag\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(text, tags)\n",
    "\n",
    "        ### compute the accuracy ###\n",
    "        preds = model.decode(text)\n",
    "        preds = nn.utils.rnn.pad_sequence([torch.tensor(x) for x in preds], batch_first=True, padding_value=PAD_IDX)\n",
    "        preds = preds.flatten() ### preds = [batch_size*seq_len]\n",
    "        preds = preds.to(device)\n",
    "        labels = tags[:, 1:].flatten() ### preds = [batch_size*seq_len]\n",
    "        assert len(preds) == len(labels)    \n",
    "\n",
    "        acc = categorical_accuracy(preds, labels, PAD_IDX)\n",
    "\n",
    "\n",
    "        loss.backward() ## back propagation\n",
    "        optimizer.step() ## update parameters\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WtNBTiPnW_X_"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text = batch.text\n",
    "            tags = batch.udtags[:,:-1] # strip <SOS> and <EOS> tag\n",
    "\n",
    "            loss = model(text, tags)\n",
    "            ### compute the accuracy ###\n",
    "            preds = model.decode(text)\n",
    "            preds = nn.utils.rnn.pad_sequence([torch.tensor(x) for x in preds], batch_first=True, padding_value=PAD_IDX)\n",
    "            preds = preds.flatten()\n",
    "            preds = preds.to(device)\n",
    "            labels = tags[:, 1:].flatten()\n",
    "            assert len(preds) == len(labels)\n",
    "\n",
    "            acc = categorical_accuracy(preds, labels, PAD_IDX)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GPKicellXTgn"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "_K5r97QkXWxY",
    "outputId": "b1cc8565-de10-478e-99ca-20795b9c4e95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Epoch Time: 0m 50s\n",
      "\tTrain Loss: 18.945 | Train Acc: 63.16%\n",
      "\t Val. Loss: 6.258 |  Val. Acc: 78.91%\n",
      "Epoch: 2 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 6.216 | Train Acc: 84.89%\n",
      "\t Val. Loss: 4.538 |  Val. Acc: 82.09%\n",
      "Epoch: 3 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 4.580 | Train Acc: 88.09%\n",
      "\t Val. Loss: 3.964 |  Val. Acc: 82.79%\n",
      "Epoch: 4 | Epoch Time: 0m 48s\n",
      "\tTrain Loss: 3.745 | Train Acc: 89.83%\n",
      "\t Val. Loss: 3.815 |  Val. Acc: 83.39%\n",
      "Epoch: 5 | Epoch Time: 0m 48s\n",
      "\tTrain Loss: 3.293 | Train Acc: 90.85%\n",
      "\t Val. Loss: 3.498 |  Val. Acc: 84.00%\n",
      "Epoch: 6 | Epoch Time: 0m 48s\n",
      "\tTrain Loss: 2.885 | Train Acc: 91.85%\n",
      "\t Val. Loss: 3.583 |  Val. Acc: 83.85%\n",
      "Epoch: 7 | Epoch Time: 0m 48s\n",
      "\tTrain Loss: 2.598 | Train Acc: 92.49%\n",
      "\t Val. Loss: 3.295 |  Val. Acc: 84.21%\n",
      "Epoch: 8 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 2.398 | Train Acc: 92.59%\n",
      "\t Val. Loss: 3.361 |  Val. Acc: 84.17%\n",
      "Epoch: 9 | Epoch Time: 0m 48s\n",
      "\tTrain Loss: 2.216 | Train Acc: 93.22%\n",
      "\t Val. Loss: 3.294 |  Val. Acc: 84.39%\n",
      "Epoch: 10 | Epoch Time: 0m 48s\n",
      "\tTrain Loss: 2.000 | Train Acc: 93.70%\n",
      "\t Val. Loss: 3.291 |  Val. Acc: 84.56%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "MODEL_PARAS_OBJ = 'pos_lstm_crf.pt'\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), MODEL_PARAS_OBJ)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "msPwA0AlKgNV"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VjuB127SXm3R",
    "outputId": "4b52fd65-09d5-4ae2-f82f-fe870761dcb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.971 |  Test Acc: 85.05%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(MODEL_PARAS_OBJ))\n",
    "test_loss, test_acc = evaluate(model, test_iterator)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AZd-Uy-iYMWm"
   },
   "source": [
    "## References\n",
    "\n",
    "https://github.com/bentrevett/pytorch-pos-tagging/blob/master/1%20-%20Simple%20RNN%20PoS%20Tagger.ipynb\n",
    "\n",
    "https://github.com/threelittlemonkeys/lstm-crf-pytorch"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pos_tagging_bilstm_crf_pytorch.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
